{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: evaluate in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: peft in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: transformers[sentencepiece] in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (4.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (1.24.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from transformers[sentencepiece]) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from transformers[sentencepiece]) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from transformers[sentencepiece]) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from transformers[sentencepiece]) (0.4.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from transformers[sentencepiece]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from transformers[sentencepiece]) (4.24.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: accelerate in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (0.23.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: sympy in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from huggingface-hub->accelerate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (0.41.1)\n",
      "Requirement already satisfied: peft in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (4.34.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (0.23.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from peft) (0.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.13.0->peft) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.13.0->peft) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from torch>=1.13.0->peft) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from accelerate->peft) (0.17.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from transformers->peft) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from transformers->peft) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from transformers->peft) (0.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests->transformers->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests->transformers->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests->transformers->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from requests->transformers->peft) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\carlos\\documents\\git\\llms\\.env\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece] peft tqdm \n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "\n",
    "!pip install --upgrade peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}\n",
    "\n",
    "def change_target(x):\n",
    "    if 'positive' in x or 'Positive' in x:\n",
    "        return 'positive'\n",
    "    elif 'negative' in x or 'Negative' in x:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "def test_fiqa(model, tokenizer, batch_size = 8):\n",
    "    dataset = load_dataset('pauri32/fiqa-2018')\n",
    "    dataset = dataset[\"test\"]\n",
    "    dataset = dataset.to_pandas()\n",
    "    dataset[\"output\"] = dataset['label']\n",
    "    dataset[\"instruction\"] = \"What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\"\n",
    "\n",
    "    dataset = dataset[['sentence', 'output', 'instruction']]\n",
    "    dataset.columns = ['input', 'output', 'instruction']\n",
    "    dataset[['context', 'target']] = dataset.apply(format_example, axis = 1, result_type=\"expand\")\n",
    "\n",
    "    # print example\n",
    "    print(f\"\\n\\nPrompt example:\\n{dataset['context'][1]}\\n\\n\")\n",
    "\n",
    "    context = dataset['context'].tolist()\n",
    "    total_steps = dataset.shape[0]//batch_size + 1\n",
    "    print(f\"Total len: {len(context)}. Batchsize: {batch_size}. Total steps: {total_steps}\")\n",
    "\n",
    "    out_text_list = []\n",
    "\n",
    "    for i in tqdm(range(total_steps)):\n",
    "        tmp_context = context[i* batch_size:(i+1)* batch_size]\n",
    "        tokens = tokenizer(tmp_context, return_tensors='pt', padding=True)\n",
    "        for k in tokens.keys():\n",
    "            tokens[k] = tokens[k].cuda()\n",
    "        \n",
    "        res = model.generate(**tokens)\n",
    "        res_sentences = tokenizer.batch_decode(res)\n",
    "        out_text = [o.split(\"Answer: \")[1] for o in res_sentences]\n",
    "        out_text_list += out_text\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    dataset[\"out_text\"] = out_text_list\n",
    "    dataset[\"new_target\"] = dataset[\"target\"].apply(change_target)\n",
    "    dataset[\"new_out\"] = dataset[\"out_text\"].apply(change_target)\n",
    "\n",
    "    acc = accuracy_score(dataset[\"new_target\"], dataset[\"new_out\"])\n",
    "    f1_macro = f1_score(dataset[\"new_target\"], dataset[\"new_out\"], average = \"macro\")\n",
    "    f1_micro = f1_score(dataset[\"new_target\"], dataset[\"new_out\"], average = \"micro\")\n",
    "    f1_weighted = f1_score(dataset[\"new_target\"], dataset[\"new_out\"], average = \"weighted\")\n",
    "\n",
    "    print(f\"Acc: {acc}. F1 macro: {f1_macro}. F1 micro: {f1_micro}. F1 weighted (BloombergGPT): {f1_weighted}. \")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('module'), WindowsPath('/matplotlib_inline.backend_inline')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "The following directories listed in your path were found to be non-existent: {WindowsPath('/usr/local/cuda/lib64')}\n",
      "DEBUG: Possible options found for libcudart.so: set()\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=118, Highest Compute Capability: 8.6.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Loading binary c:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.so...\n",
      "argument of type 'WindowsPath' is not iterable\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n",
      "CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n",
      "CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n",
      "CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n",
      "CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n",
      "CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n",
      "CUDA SETUP: Solution 2a): Download CUDA install script: wget https://github.com/TimDettmers/bitsandbytes/blob/main/cuda_install.sh\n",
      "CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n",
      "CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\LLM.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Carlos/Documents/git/llms/LLM.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftModel, PeftConfig\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carlos/Documents/git/llms/LLM.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Carlos/Documents/git/llms/LLM.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\peft\\__init__.py:22\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.5.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     AutoPeftModel,\n\u001b[0;32m     24\u001b[0m     AutoPeftModelForCausalLM,\n\u001b[0;32m     25\u001b[0m     AutoPeftModelForSequenceClassification,\n\u001b[0;32m     26\u001b[0m     AutoPeftModelForSeq2SeqLM,\n\u001b[0;32m     27\u001b[0m     AutoPeftModelForTokenClassification,\n\u001b[0;32m     28\u001b[0m     AutoPeftModelForQuestionAnswering,\n\u001b[0;32m     29\u001b[0m     AutoPeftModelForFeatureExtraction,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmapping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[0;32m     33\u001b[0m     PEFT_TYPE_TO_CONFIG_MAPPING,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     inject_adapter_in_model,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     PeftModel,\n\u001b[0;32m     40\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     PeftModelForFeatureExtraction,\n\u001b[0;32m     46\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\peft\\auto.py:31\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     AutoModel,\n\u001b[0;32m     23\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     AutoModelForTokenClassification,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig\n\u001b[1;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmapping\u001b[39;00m \u001b[39mimport\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     PeftModel,\n\u001b[0;32m     34\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     PeftModelForTokenClassification,\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_BaseAutoPeftModel\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\peft\\mapping.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig\n\u001b[1;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     PeftModel,\n\u001b[0;32m     25\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m     26\u001b[0m     PeftModelForFeatureExtraction,\n\u001b[0;32m     27\u001b[0m     PeftModelForQuestionAnswering,\n\u001b[0;32m     28\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[0;32m     29\u001b[0m     PeftModelForSequenceClassification,\n\u001b[0;32m     30\u001b[0m     PeftModelForTokenClassification,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     AdaLoraConfig,\n\u001b[0;32m     34\u001b[0m     AdaLoraModel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     PromptTuningConfig,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _prepare_prompt_learning_config\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\peft\\peft_model.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftConfig\n\u001b[1;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     AdaLoraModel,\n\u001b[0;32m     40\u001b[0m     AdaptionPromptModel,\n\u001b[0;32m     41\u001b[0m     IA3Model,\n\u001b[0;32m     42\u001b[0m     LoraModel,\n\u001b[0;32m     43\u001b[0m     PrefixEncoder,\n\u001b[0;32m     44\u001b[0m     PromptEmbedding,\n\u001b[0;32m     45\u001b[0m     PromptEncoder,\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     SAFETENSORS_WEIGHTS_NAME,\n\u001b[0;32m     49\u001b[0m     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     shift_tokens_right,\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     66\u001b[0m PEFT_TYPE_TO_MODEL_MAPPING \u001b[39m=\u001b[39m {\n\u001b[0;32m     67\u001b[0m     PeftType\u001b[39m.\u001b[39mLORA: LoraModel,\n\u001b[0;32m     68\u001b[0m     PeftType\u001b[39m.\u001b[39mPROMPT_TUNING: PromptEmbedding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m     PeftType\u001b[39m.\u001b[39mIA3: IA3Model,\n\u001b[0;32m     74\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\peft\\tuners\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# module, but to preserve other warnings. So, don't check this module at all\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madaption_prompt\u001b[39;00m \u001b[39mimport\u001b[39;00m AdaptionPromptConfig, AdaptionPromptModel\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlora\u001b[39;00m \u001b[39mimport\u001b[39;00m LoraConfig, LoraModel\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mia3\u001b[39;00m \u001b[39mimport\u001b[39;00m IA3Config, IA3Model\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madalora\u001b[39;00m \u001b[39mimport\u001b[39;00m AdaLoraConfig, AdaLoraModel\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\peft\\tuners\\lora.py:45\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseTuner, BaseTunerLayer\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m is_bnb_available():\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbnb\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m@dataclass\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLoraConfig\u001b[39;00m(PeftConfig):\n\u001b[0;32m     50\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m    This is the configuration class to store the configuration of a [`LoraModel`].\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m            pattern is not in the common layers pattern.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\bitsandbytes\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     MatmulLtState,\n\u001b[0;32m      9\u001b[0m     bmm_cublas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     matmul_4bit\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcextension\u001b[39;00m \u001b[39mimport\u001b[39;00m COMPILED_WITH_CUDA\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\bitsandbytes\\research\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     switchback_bnb,\n\u001b[0;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[0;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\bitsandbytes\\research\\nn\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\bitsandbytes\\research\\nn\\modules.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor, device, dtype, nn\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbnb\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m GlobalOptimManager\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n\u001b[0;32m     11\u001b[0m T \u001b[39m=\u001b[39m TypeVar(\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, bound\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorch.nn.Module\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\bitsandbytes\\optim\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcextension\u001b[39;00m \u001b[39mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madagrad\u001b[39;00m \u001b[39mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madam\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam, Adam8bit, Adam32bit, PagedAdam, PagedAdam8bit, PagedAdam32bit\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\bitsandbytes\\cextension.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m     CUDASetup\u001b[39m.\u001b[39mget_instance()\u001b[39m.\u001b[39mgenerate_instructions()\n\u001b[0;32m     19\u001b[0m     CUDASetup\u001b[39m.\u001b[39mget_instance()\u001b[39m.\u001b[39mprint_log_stack()\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'''\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[39m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[39m    python -m bitsandbytes\u001b[39m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[39m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[39m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[39m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[39m'''\u001b[39m)\n\u001b[0;32m     28\u001b[0m lib\u001b[39m.\u001b[39mcadam32bit_grad_fp32 \u001b[39m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n\u001b[0;32m     29\u001b[0m lib\u001b[39m.\u001b[39mget_context\u001b[39m.\u001b[39mrestype \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39mc_void_p\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\"\n",
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "peft_model = \"FinGPT/fingpt-mt_llama2-7b_lora\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, load_in_8bit=True, device_map=device)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model)\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}. \n",
      "For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m \n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "instruction = \"What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\"\n",
    "news = \"For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m\" \n",
    "\n",
    "instruction = f\"{instruction} \\n{news} \\nAnswer: \"\n",
    "print(instruction)\n",
    "\n",
    "model_input = tokenizer(instruction, return_tensors=\"pt\")\n",
    "model_input = model_input.to(device)\n",
    "\n",
    "res = model.generate(**model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}. \\nFor the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m \\nAnswer:  neutral</s>\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_sentences = [tokenizer.decode(i) for i in res]\n",
    "res_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}. \\nFor the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m \\nAnswer:  neutral</s>\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.batch_decode(res)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prompt example:\n",
      "Instruction: What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.\n",
      "Input: @gakrum nice chart shows distinctive down channel not a dip.. where do you see the bottom? $SPY ..$150? ..$130?\n",
      "Answer: \n",
      "\n",
      "\n",
      "Total len: 150. Batchsize: 8. Total steps: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\LLM.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Carlos/Documents/git/llms/LLM.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_fiqa(model, tokenizer)\n",
      "\u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\LLM.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Carlos/Documents/git/llms/LLM.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(total_steps)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Carlos/Documents/git/llms/LLM.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     tmp_context \u001b[39m=\u001b[39m context[i\u001b[39m*\u001b[39m batch_size:(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m batch_size]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Carlos/Documents/git/llms/LLM.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     tokens \u001b[39m=\u001b[39m tokenizer(tmp_context, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Carlos/Documents/git/llms/LLM.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m tokens\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Carlos/Documents/git/llms/LLM.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         tokens[k] \u001b[39m=\u001b[39m tokens[k]\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2790\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2788\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2789\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2790\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[0;32m   2791\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2792\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2876\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2871\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2872\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2873\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2874\u001b[0m         )\n\u001b[0;32m   2875\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2876\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_encode_plus(\n\u001b[0;32m   2877\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39;49mbatch_text_or_text_pairs,\n\u001b[0;32m   2878\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   2879\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   2880\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m   2881\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   2882\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   2883\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   2884\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   2885\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   2886\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   2887\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   2888\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   2889\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   2890\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   2891\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   2892\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   2893\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2894\u001b[0m     )\n\u001b[0;32m   2895\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2896\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2897\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2898\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2914\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2915\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3058\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3041\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3042\u001b[0m \u001b[39mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[0;32m   3043\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3054\u001b[0m \u001b[39m        details in `encode_plus`).\u001b[39;00m\n\u001b[0;32m   3055\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3057\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m-> 3058\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_padding_truncation_strategies(\n\u001b[0;32m   3059\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   3060\u001b[0m     truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m   3061\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   3062\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   3063\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   3064\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   3065\u001b[0m )\n\u001b[0;32m   3067\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3068\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3069\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3084\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3085\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Carlos\\Documents\\git\\llms\\.env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2695\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[1;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2693\u001b[0m \u001b[39m# Test if we have a padding token\u001b[39;00m\n\u001b[0;32m   2694\u001b[0m \u001b[39mif\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad_token_id \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m-> 2695\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2696\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsking to pad but the tokenizer does not have a padding token. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2697\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2698\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor add a new pad token via `tokenizer.add_special_tokens(\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpad_token\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m\u001b[39m})`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2699\u001b[0m     )\n\u001b[0;32m   2701\u001b[0m \u001b[39m# Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2703\u001b[0m     truncation_strategy \u001b[39m!=\u001b[39m TruncationStrategy\u001b[39m.\u001b[39mDO_NOT_TRUNCATE\n\u001b[0;32m   2704\u001b[0m     \u001b[39mand\u001b[39;00m padding_strategy \u001b[39m!=\u001b[39m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2707\u001b[0m     \u001b[39mand\u001b[39;00m (max_length \u001b[39m%\u001b[39m pad_to_multiple_of \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m   2708\u001b[0m ):\n",
      "\u001b[1;31mValueError\u001b[0m: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`."
     ]
    }
   ],
   "source": [
    "test_fiqa(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
